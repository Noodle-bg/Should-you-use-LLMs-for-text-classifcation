{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DIafwumGB7yL","outputId":"f60fbeb7-2cda-4e09-a804-54375a31e2ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","NUM_LABELS = 20\n","# print(len(combined_dataset))\n","\n","id2label = {\n","    0: \"alt.atheism\",\n","    1: \"comp.graphics\",\n","    2: \"comp.os.ms-windows.misc\",\n","    3: \"comp.sys.ibm.pc.hardware\",\n","    4: \"comp.sys.mac.hardware\",\n","    5: \"comp.windows.x\",\n","    6: \"misc.forsale\",\n","    7: \"rec.autos\",\n","    8: \"rec.motorcycles\",\n","    9: \"rec.sport.baseball\",\n","    10: \"rec.sport.hockey\",\n","    11: \"sci.crypt\",\n","    12: \"sci.electronics\",\n","    13: \"sci.med\",\n","    14: \"sci.space\",\n","    15: \"soc.religion.christian\",\n","    16: \"talk.politics.guns\",\n","    17: \"talk.politics.mideast\",\n","    18: \"talk.politics.misc\",\n","    19: \"talk.religion.misc\"\n","}\n","\n","label2id = {\n","    \"alt.atheism\": 0,\n","    \"comp.graphics\": 1,\n","    \"comp.os.ms-windows.misc\": 2,\n","    \"comp.sys.ibm.pc.hardware\": 3,\n","    \"comp.sys.mac.hardware\": 4,\n","    \"comp.windows.x\": 5,\n","    \"misc.forsale\": 6,\n","    \"rec.autos\": 7,\n","    \"rec.motorcycles\": 8,\n","    \"rec.sport.baseball\": 9,\n","    \"rec.sport.hockey\": 10,\n","    \"sci.crypt\": 11,\n","    \"sci.electronics\": 12,\n","    \"sci.med\": 13,\n","    \"sci.space\": 14,\n","    \"soc.religion.christian\": 15,\n","    \"talk.politics.guns\": 16,\n","    \"talk.politics.mideast\": 17,\n","    \"talk.politics.misc\": 18,\n","    \"talk.religion.misc\": 19\n","}\n","dataset = load_dataset(\"json\", data_files=\"train_dataset_20_newsgroups_noisy.json\", split=\"train\")\n","train_texts=[]\n","train_labels = []\n","for stuff in dataset:\n","    train_texts.append(stuff[\"messages\"][1][\"content\"])\n","    train_labels.append(id2label[int(stuff[\"messages\"][2][\"content\"])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9MsjOYSB7yN"},"outputs":[],"source":["dataset = load_dataset(\"json\", data_files=\"test_dataset_20_newsgroups.json\", split=\"train\")\n","test_texts=[]\n","test_labels = []\n","for stuff in dataset:\n","    test_texts.append(stuff[\"messages\"][1][\"content\"])\n","    test_labels.append(id2label[int(stuff[\"messages\"][2][\"content\"])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTBt14zsB7yO","outputId":"e3f73825-7be6-4e41-98f9-a7b80a820cb4"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/trainee/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/trainee/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/trainee/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/trainee/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from collections import defaultdict\n","from nltk.corpus import wordnet as wn\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import model_selection, naive_bayes, svm\n","from sklearn.metrics import accuracy_score\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download(\"wordnet\")\n","\n","# Step - a : Remove blank rows if any.\n","train_texts = [text for text in train_texts if text.strip()]\n","\n","# Step - b : Change all the text to lower case.\n","train_texts = [text.lower() for text in train_texts]\n","\n","# Step - c : Tokenization\n","train_texts = [[word for word in word_tokenize(text)] for text in train_texts]\n","\n","# Step - d : Remove Stop words, Non-Numeric and perform Word Stemming/Lemmatization\n","tag_map = defaultdict(lambda: wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV\n","\n","train_texts_final = []\n","for entry in train_texts:\n","    final_words = []\n","    word_lemmatizer = WordNetLemmatizer()\n","    for word, tag in pos_tag(entry):\n","        if word not in stopwords.words('english') and word.isalpha():\n","            word_final = word_lemmatizer.lemmatize(word, tag_map[tag[0]])\n","            final_words.append(word_final)\n","    train_texts_final.append(final_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rUiW-VOB7yO"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download(\"wordnet\")\n","\n","# Step - a : Remove blank rows if any.\n","test_texts = [text for text in test_texts if text.strip()]\n","\n","# Step - b : Change all the text to lower case.\n","test_texts = [text.lower() for text in test_texts]\n","\n","# Step - c : Tokenization\n","test_texts = [[word for word in word_tokenize(text)] for text in test_texts]\n","\n","# Step - d : Remove Stop words, Non-Numeric and perform Word Stemming/Lemmatization\n","tag_map = defaultdict(lambda: wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV\n","\n","test_texts_final = []\n","for entry in test_texts:\n","    final_words = []\n","    word_lemmatizer = WordNetLemmatizer()\n","    for word, tag in pos_tag(entry):\n","        if word not in stopwords.words('english') and word.isalpha():\n","            word_final = word_lemmatizer.lemmatize(word, tag_map[tag[0]])\n","            final_words.append(word_final)\n","    test_texts_final.append(final_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1un-PsMB7yO"},"outputs":[],"source":["Encoder = LabelEncoder()\n","Train_Y = Encoder.fit_transform(train_labels)\n","Test_Y = Encoder.fit_transform(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjD4yIdlB7yO"},"outputs":[],"source":["train_texts_final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TZq5jttB7yP"},"outputs":[],"source":["\n","Tfidf_vect = TfidfVectorizer(max_features=5000)\n","\n","# Flatten the train_texts_final and test_texts_final lists\n","train_texts_flat = [' '.join(text) for text in train_texts_final]\n","test_texts_flat = [' '.join(text) for text in test_texts_final]\n","\n","Tfidf_vect.fit(train_texts_flat)\n","Train_X_Tfidf = Tfidf_vect.transform(train_texts_flat)\n","Test_X_Tfidf = Tfidf_vect.transform(test_texts_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFijATxkB7yP","outputId":"92f5241d-774a-4790-9c91-5800899efce5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Naive Bayes Accuracy Score (Clean) ->  88.58038513210927\n"]}],"source":["# fit the training dataset on the NB classifier\n","Naive = naive_bayes.MultinomialNB()\n","Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n","predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n","print(\"Naive Bayes Accuracy Score (Noisy)-> \",accuracy_score(predictions_NB, Test_Y)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BKdXj4KB7yP","outputId":"a43e131f-62db-4f3d-d5dd-dc419dc86b0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM Accuracy Score (Clean) ->  85.7590685176892\n"]}],"source":["# Classifier - Algorithm - SVM\n","# fit the training dataset on the classifier\n","SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n","SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n","predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n","print(\"SVM Accuracy Score (Noisy)-> \",accuracy_score(predictions_SVM, Test_Y)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK9ZQYM0B7yP"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","parameters = {'C': [1, 10],\n","          'gamma': [0.001, 0.01, 1]}\n","model = svm.SVC()\n","grid = GridSearchCV(estimator=model, param_grid=parameters)\n","grid.fit(Train_X_Tfidf, Train_Y)\n","predictions_SVM_grid =grid.predict(Test_X_Tfidf)\n","print(grid)\n","# summarize the results of the grid search\n","print(grid.best_score_)\n","print(grid.best_estimator_)\n","\n","print(\"--------------------------\")\n","print(f\"SVM Grid Acc ->{accuracy_score(predictions_SVM_grid,Test_Y)*100}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}