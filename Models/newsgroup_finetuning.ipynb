{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:56:37.536081Z","iopub.status.busy":"2024-07-04T12:56:37.534930Z","iopub.status.idle":"2024-07-04T12:57:45.934649Z","shell.execute_reply":"2024-07-04T12:57:45.933179Z"},"id":"2IbA6m-X9xdl"},"outputs":[],"source":["# Install Pytorch & other libraries\n","!pip install \"torch==2.1.2\" tensorboard\n","\n","# Install Hugging Face libraries\n","!pip install  --upgrade \\\n","  \"datasets==2.16.1\" \\\n","  \"accelerate==0.26.1\" \\\n","  \"evaluate==0.4.1\" \\\n","  \"bitsandbytes==0.42.0\" \\\n","  # \"trl==0.7.10\" # \\\n","  # \"peft==0.7.1\" \\\n","  #\n","\n","# install peft & trl from github\n","!pip install git+https://github.com/huggingface/trl.git\n","!pip install git+https://github.com/huggingface/peft.git\n","! pip install --no-cache git+https://github.com/huggingface/transformers.git"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:57:45.941089Z","iopub.status.busy":"2024-07-04T12:57:45.940591Z","iopub.status.idle":"2024-07-04T12:57:45.946352Z","shell.execute_reply":"2024-07-04T12:57:45.945176Z"},"id":"vwOrzuGh9xds"},"outputs":[],"source":["# from trl import setup_chat_format, SFTTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:57:45.951627Z","iopub.status.busy":"2024-07-04T12:57:45.951061Z","iopub.status.idle":"2024-07-04T12:57:46.077445Z","shell.execute_reply":"2024-07-04T12:57:46.076834Z"},"id":"LT6BkJnL9xds"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:57:46.133741Z","iopub.status.busy":"2024-07-04T12:57:46.132927Z","iopub.status.idle":"2024-07-04T12:58:22.063892Z","shell.execute_reply":"2024-07-04T12:58:22.063170Z"},"colab":{"referenced_widgets":["daf61e38f81c4c98b1e86a1b5c3c0f44","8136044c2d51444fa4053dfd4f973d75"]},"id":"lqMiTIJV9xdw","outputId":"cb067890-15ed-46ab-cc58-d504684dadc6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (2881 > 2048). Running this sequence through the model will result in indexing errors\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"daf61e38f81c4c98b1e86a1b5c3c0f44","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10019 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8136044c2d51444fa4053dfd4f973d75","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6707 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'attention_mask'],\n","    num_rows: 10019\n","}) Dataset({\n","    features: ['text', 'label', 'input_ids', 'attention_mask'],\n","    num_rows: 6707\n","})\n"]}],"source":["from transformers import AutoTokenizer\n","from datasets import load_dataset, Dataset\n","\n","# Load dataset\n","combined_dataset = load_dataset(\"SetFit/20_newsgroups\")\n","train_data = combined_dataset[\"train\"]\n","test_data = combined_dataset[\"test\"]\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\",\n","                                              use_fast=False,\n","                                            trust_remote_code=True,\n","                                            pad_token=\"<|endoftext|>\")\n","if tokenizer.pad_token_id is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# Set maximum token length\n","max_token_length = 512\n","\n","# Filter data by max token length\n","train_data = [example for example in train_data if len(tokenizer(example[\"text\"])[\"input_ids\"]) <= max_token_length]\n","test_data = [example for example in test_data if len(tokenizer(example[\"text\"])[\"input_ids\"]) <= max_token_length]\n","\n","# Convert filtered lists to dictionaries suitable for Dataset.from_dict()\n","train_dict = {\"text\": [example[\"text\"] for example in train_data], \"label\": [example[\"label\"] for example in train_data]}\n","eval_dict = {\"text\": [example[\"text\"] for example in test_data], \"label\": [example[\"label\"] for example in test_data]}\n","\n","# Create Dataset objects\n","train_dataset = Dataset.from_dict(train_dict)\n","eval_dataset = Dataset.from_dict(eval_dict)\n","# Tokenization and padding function\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_token_length)\n","\n","# Tokenize and pad datasets\n","tokenized_dataset_train = train_dataset.map(tokenize_function, batched=True)\n","tokenized_dataset_test = eval_dataset.map(tokenize_function, batched=True)\n","\n","# Print the tokenized datasets\n","print(tokenized_dataset_train, tokenized_dataset_test)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:22.081526Z","iopub.status.busy":"2024-07-04T12:58:22.081283Z","iopub.status.idle":"2024-07-04T12:58:22.086752Z","shell.execute_reply":"2024-07-04T12:58:22.086029Z"},"id":"fMYtXzli9xdx"},"outputs":[],"source":["NUM_LABELS = 20\n","# print(len(combined_dataset))\n","\n","id2label = {\n","    0: \"alt.atheism\",\n","    1: \"comp.graphics\",\n","    2: \"comp.os.ms-windows.misc\",\n","    3: \"comp.sys.ibm.pc.hardware\",\n","    4: \"comp.sys.mac.hardware\",\n","    5: \"comp.windows.x\",\n","    6: \"misc.forsale\",\n","    7: \"rec.autos\",\n","    8: \"rec.motorcycles\",\n","    9: \"rec.sport.baseball\",\n","    10: \"rec.sport.hockey\",\n","    11: \"sci.crypt\",\n","    12: \"sci.electronics\",\n","    13: \"sci.med\",\n","    14: \"sci.space\",\n","    15: \"soc.religion.christian\",\n","    16: \"talk.politics.guns\",\n","    17: \"talk.politics.mideast\",\n","    18: \"talk.politics.misc\",\n","    19: \"talk.religion.misc\"\n","}\n","\n","label2id = {\n","    \"alt.atheism\": 0,\n","    \"comp.graphics\": 1,\n","    \"comp.os.ms-windows.misc\": 2,\n","    \"comp.sys.ibm.pc.hardware\": 3,\n","    \"comp.sys.mac.hardware\": 4,\n","    \"comp.windows.x\": 5,\n","    \"misc.forsale\": 6,\n","    \"rec.autos\": 7,\n","    \"rec.motorcycles\": 8,\n","    \"rec.sport.baseball\": 9,\n","    \"rec.sport.hockey\": 10,\n","    \"sci.crypt\": 11,\n","    \"sci.electronics\": 12,\n","    \"sci.med\": 13,\n","    \"sci.space\": 14,\n","    \"soc.religion.christian\": 15,\n","    \"talk.politics.guns\": 16,\n","    \"talk.politics.mideast\": 17,\n","    \"talk.politics.misc\": 18,\n","    \"talk.religion.misc\": 19\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:22.115639Z","iopub.status.busy":"2024-07-04T12:58:22.115059Z","iopub.status.idle":"2024-07-04T12:58:22.362406Z","shell.execute_reply":"2024-07-04T12:58:22.360867Z"},"id":"G77KTIYP9xdy"},"outputs":[],"source":["! nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:22.368446Z","iopub.status.busy":"2024-07-04T12:58:22.367921Z","iopub.status.idle":"2024-07-04T12:58:25.152783Z","shell.execute_reply":"2024-07-04T12:58:25.151530Z"},"id":"SmtcW4Mj9xdy"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification,LlamaTokenizer, LlamaForCausalLM,LlamaForSequenceClassification\n","# from trl import setup_chat_format\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","\n","# Hugging Face model id\n","model_id = \"openlm-research/open_llama_3b_v2\" # or `mistralai/Mistral-7B-v0.1`\n","\n","# BitsAndBytesConfig int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","# Load model and tokenizer\n","model = LlamaForSequenceClassification.from_pretrained(\n","    model_id,\n","    num_labels= 20,\n","    torch_dtype = torch.float16,\n","    device_map=\"auto\"\n","\n",")\n","\n","\n","\n","\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:25.171852Z","iopub.status.busy":"2024-07-04T12:58:25.171552Z","iopub.status.idle":"2024-07-04T12:58:25.177182Z","shell.execute_reply":"2024-07-04T12:58:25.176420Z"},"id":"5lPRk8E89xdy"},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:25.182015Z","iopub.status.busy":"2024-07-04T12:58:25.181628Z","iopub.status.idle":"2024-07-04T12:58:26.063821Z","shell.execute_reply":"2024-07-04T12:58:26.063356Z"},"id":"uHL6bh8v9xdy","outputId":"0aebe650-1fa7-4abf-e477-c40ab0ca01cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 25,541,440 || all params: 3,349,733,760 || trainable%: 0.7625\n"]}],"source":["from peft import LoraConfig\n","\n","# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n","peft_config = LoraConfig(\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        r=16,\n","        bias=\"none\",\n","        target_modules=\"all-linear\",\n","        task_type=\"SEQ_CLS\",\n",")\n","model.gradient_checkpointing_enable()\n","peft_model = get_peft_model(model, peft_config)\n","peft_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:26.066554Z","iopub.status.busy":"2024-07-04T12:58:26.066192Z","iopub.status.idle":"2024-07-04T12:58:26.079011Z","shell.execute_reply":"2024-07-04T12:58:26.078490Z"},"id":"B_dnQ5Rd9xdz","outputId":"18cdc4be-22c6-4463-fbf1-353314923f70"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/transformers/training_args.py:1504: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["# del model\n","from transformers import TrainingArguments\n","\n","\n","args = TrainingArguments(\n","    output_dir=\"openlm-research/open_llama_3b_v2_new_newsgroup_full\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"adamw_torch_fused\",\n","    logging_steps=100,\n","    # save_strategy=\"epoch\",\n","    warmup_steps=500,\n","    bf16=False,\n","    tf32=False,\n","    push_to_hub=True,\n","    report_to=\"tensorboard\",\n","    disable_tqdm=False,\n","    # load_best_model_at_end=True,\n","    # metric_for_best_model=\"accuracy\",\n","    evaluation_strategy=\"no\"  # Disable evaluation during training\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:26.089205Z","iopub.status.busy":"2024-07-04T12:58:26.088966Z","iopub.status.idle":"2024-07-04T12:58:26.659463Z","shell.execute_reply":"2024-07-04T12:58:26.658124Z"},"id":"q4DRHi2a9xdz"},"outputs":[],"source":["from transformers import Trainer\n","del model\n","trainer = Trainer(\n","    model=peft_model,\n","    args=args,\n","    train_dataset=tokenized_dataset_train,\n","    # eval_dataset=tokenized_dataset_test,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T12:58:26.675754Z","iopub.status.busy":"2024-07-04T12:58:26.675161Z","iopub.status.idle":"2024-07-04T22:38:24.153455Z","shell.execute_reply":"2024-07-04T22:38:24.152247Z"},"id":"kUQ-pCPk9xdz","outputId":"1b6c4dd4-3ea5-491d-ee25-25945a807bea"},"outputs":[{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3756' max='3756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3756/3756 9:34:51, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>3.763100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.219500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.567100</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.476900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.385400</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.201600</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.042500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.068800</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.966800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.902400</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.879500</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.881200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.748100</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.658200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.597600</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.630300</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.605200</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.602200</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.666000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.586400</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.598000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.675300</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.510500</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.565200</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.560900</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.346900</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.210700</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.246600</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.266800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.279800</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.264000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.268600</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.255500</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.251400</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.262500</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.276500</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.270700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/trainee/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Noodle-bg/open_llama_3b_v2_new_newsgroup_full/commit/2b425ada214d4ad4c6810bf4768185a4f90ff61c', commit_message='openlm-research/open_llama_3b_v2-Classifier_new', commit_description='', oid='2b425ada214d4ad4c6810bf4768185a4f90ff61c', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","trainer.train()\n","trainer.save_model(\"./openlm-research/open_llama_3b_v2-Text-Classifier_new\")\n","trainer.push_to_hub(\"openlm-research/open_llama_3b_v2-Classifier_new\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.172049Z","iopub.status.busy":"2024-07-04T22:38:24.171403Z","iopub.status.idle":"2024-07-04T22:38:24.176956Z","shell.execute_reply":"2024-07-04T22:38:24.175779Z"},"id":"vPDYDtWh9xd0"},"outputs":[],"source":["# tokenized_dataset_train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.182608Z","iopub.status.busy":"2024-07-04T22:38:24.182115Z","iopub.status.idle":"2024-07-04T22:38:24.187411Z","shell.execute_reply":"2024-07-04T22:38:24.186270Z"},"id":"o1Kk5t3Z9xd0"},"outputs":[],"source":["# import torch\n","# torch.cuda.empty_cache()\n","# results = trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.193164Z","iopub.status.busy":"2024-07-04T22:38:24.192675Z","iopub.status.idle":"2024-07-04T22:38:24.537212Z","shell.execute_reply":"2024-07-04T22:38:24.535579Z"},"id":"5lSitESy9xd0","outputId":"8fad5f91-406c-4b15-a94a-b8bf3a54e007"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Jul  5 04:08:24 2024       \r\n","+-----------------------------------------------------------------------------------------+\r\n","| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\r\n","|-----------------------------------------+------------------------+----------------------+\r\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n","|                                         |                        |               MIG M. |\r\n","|=========================================+========================+======================|\r\n"]},{"name":"stdout","output_type":"stream","text":["|   0  Quadro RTX 6000                Off |   00000000:65:00.0 Off |                  Off |\r\n","| 33%   51C    P2             69W /  260W |   14968MiB /  24576MiB |      0%      Default |\r\n","|                                         |                        |                  N/A |\r\n","+-----------------------------------------+------------------------+----------------------+\r\n","                                                                                         \r\n","+-----------------------------------------------------------------------------------------+\r\n","| Processes:                                                                              |\r\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n","|        ID   ID                                                               Usage      |\r\n","|=========================================================================================|\r\n","|    0   N/A  N/A      2571      G   /usr/lib/xorg/Xorg                             99MiB |\r\n","|    0   N/A  N/A      2764      G   /usr/bin/gnome-shell                           73MiB |\r\n","|    0   N/A  N/A    893276      C   /usr/bin/python3                            14766MiB |\r\n","+-----------------------------------------------------------------------------------------+\r\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.543670Z","iopub.status.busy":"2024-07-04T22:38:24.542942Z","iopub.status.idle":"2024-07-04T22:38:24.548805Z","shell.execute_reply":"2024-07-04T22:38:24.547709Z"},"id":"TWvUKdqh9xd1"},"outputs":[],"source":["# # free the memory again\n","# # del model\n","# del trainer\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.554324Z","iopub.status.busy":"2024-07-04T22:38:24.553694Z","iopub.status.idle":"2024-07-04T22:38:24.560891Z","shell.execute_reply":"2024-07-04T22:38:24.559736Z"},"id":"g3pehHfg9xd1"},"outputs":[],"source":["# import torch\n","# from peft import AutoPeftModelForCausalLM\n","# from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n","# from peft import PeftModel, PeftConfig\n","# from trl import setup_chat_format\n","# peft_model_id =\"code-llama-7b-Text-classification-20_newsgroup\"\n","# # peft_model_id = args.output_dir\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n","# )\n","\n","# # # Load Model with PEFT adapter\n","# # model = PeftModel.from_pretrained(\n","# #   peft_model_id,\n","# #   device_map=\"auto\",\n","# #   torch_dtype=torch.float16,\n","# #   quantization_config=bnb_config\n","# # )\n","# model = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_3b_v2\",\n","#                                                     device_map=\"auto\",\n","#                                                     torch_dtype=torch.float16,\n","#                                                     quantization_config=bnb_config)\n","# tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n","# tokenizer.padding_side= 'right'\n","# model, tokenizer = setup_chat_format(model, tokenizer)\n","# model = PeftModel.from_pretrained(model, \"open_llama_3b_v2-20_newsgroup_full\")\n","\n","# # load into pipeline\n","# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dTPcHFEt_PLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.566479Z","iopub.status.busy":"2024-07-04T22:38:24.565990Z","iopub.status.idle":"2024-07-04T22:38:24.571611Z","shell.execute_reply":"2024-07-04T22:38:24.570428Z"},"id":"Nm1-NTuY9xd1"},"outputs":[],"source":["# pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.577148Z","iopub.status.busy":"2024-07-04T22:38:24.576503Z","iopub.status.idle":"2024-07-04T22:38:24.582592Z","shell.execute_reply":"2024-07-04T22:38:24.581447Z"},"id":"VVW_KFdC9xd1"},"outputs":[],"source":["# from datasets import load_dataset\n","# from random import randint\n","# # del model\n","# # Load our test dataset\n","# eval_dataset = load_dataset(\"json\", data_files=\"test_dataset_20_newsgroups.json\", split=\"train\")\n","# rand_idx = randint(0, len(eval_dataset))\n","\n","# # Test on sample\n","# prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","# outputs = pipe(prompt, top_k=50, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","\n","# print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n","# print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n","# print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.588049Z","iopub.status.busy":"2024-07-04T22:38:24.587550Z","iopub.status.idle":"2024-07-04T22:38:24.594878Z","shell.execute_reply":"2024-07-04T22:38:24.593516Z"},"id":"eXnsY-Ui9xd1"},"outputs":[],"source":["# from tqdm import tqdm\n","# from datasets import load_dataset\n","\n","\n","# def evaluate(sample):\n","#     prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","#     outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","#     predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n","#     if int(predicted_answer) == int(sample[\"messages\"][2][\"content\"]):\n","#         return 1\n","#     else:\n","#         return 0\n","\n","# success_rate = []\n","# number_of_eval_samples = 1000\n","# # iterate over eval dataset and predict\n","# eval_dataset = load_dataset(\"json\", data_files=\"test_dataset_20_newsgroups.json\", split=\"train\").shuffle().select(range(number_of_eval_samples))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T22:38:24.600601Z","iopub.status.busy":"2024-07-04T22:38:24.599952Z","iopub.status.idle":"2024-07-04T22:38:24.605611Z","shell.execute_reply":"2024-07-04T22:38:24.604246Z"},"id":"Yz5yxQeI9xd1"},"outputs":[],"source":["# with torch.no_grad():\n","#     for s in eval_dataset:\n","#         success_rate.append(evaluate(s))\n","\n","# # compute accuracy\n","# accuracy = sum(success_rate)/len(success_rate)\n","\n","# print(f\"Accuracy: {accuracy*100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m56vaFi-9xd2"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}